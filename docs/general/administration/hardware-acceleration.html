<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Hardware Acceleration | Documentation - Jellyfin Project </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Hardware Acceleration | Documentation - Jellyfin Project ">
    <meta name="generator" content="docfx 2.48.1.0">
    
    <link rel="shortcut icon" href="../../images/favicon.png">
    <link rel="stylesheet" href="../../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../../styles/docfx.css">
    <link rel="stylesheet" href="../../styles/main.css">
    <meta property="docfx:navrel" content="../../toc.html">
    <meta property="docfx:tocrel" content="../toc.html">
    
    <meta property="docfx:rel" content="../../">
    
  </head>
  <body data-spy="scroll" data-target="#affix" data-offset="120">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              
              <a class="navbar-brand" href="../../index.html">
                <img id="logo" class="svg" src="../../images/header-icon.svg" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>
        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div class="container body-content">
        
        <div id="search-results">
          <div class="search-list"></div>
          <div class="sr-items">
            <p><i class="glyphicon glyphicon-refresh index-loading"></i></p>
          </div>
          <ul id="pagination"></ul>
        </div>
      </div>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="admin-hardware-acceleration">
<h1 id="hardware-acceleration">Hardware Acceleration</h1>

<p>Jellyfin supports <a href="https://trac.ffmpeg.org/wiki/HWAccelIntro">hardware acceleration</a> (HWA) of video encoding/decoding using FFMpeg. FFMpeg and Jellyfin can support multiple hardware acceleration implementations such as Intel Quicksync (QSV), AMD AMF, nVidia NVENC/NVDEC, OpenMax OMX and MediaCodec through Video Acceleration API&#39;s.</p>
<p><a href="https://en.wikipedia.org/wiki/Video_Acceleration_API">VAAPI</a> is a Video Acceleration API that uses <a href="https://github.com/intel/libva/blob/master/README.md">libva</a> to interface with local drivers to provide HWA. <a href="https://trac.ffmpeg.org/wiki/Hardware/QuickSync">QSV</a> uses a modified (forked) version of VAAPI and interfaces it with <a href="https://github.com/intel/media-driver/blob/master/README.md">libmfx</a> and their proprietary drivers <a href="https://ark.intel.com/content/www/us/en/ark.html#@Processors">(list of supported processors for QSV)</a>.</p>
<table>
<thead>
<tr>
<th>OS</th>
<th>Recommended HW Acceleration</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux</td>
<td>QSV, NVENC, AMF, VAAPI</td>
</tr>
<tr>
<td>Windows</td>
<td>QSV, NVENC, AMF</td>
</tr>
<tr>
<td>MacOS</td>
<td>VideoToolbox</td>
</tr>
<tr>
<td>Android</td>
<td>MediaCodec, OMX</td>
</tr>
<tr>
<td>RPi</td>
<td>OMX, VAAPI</td>
</tr>
</tbody>
</table>
<p><a href="https://www.elpamsoft.com/?p=Plex-Hardware-Transcoding">Graphics Cards comparison using HWA</a></p>
<h2 id="nvidia-nvenc">NVIDIA NVENC</h2>
<p><a href="https://developer.nvidia.com/ffmpeg">NVIDIA using ffmpeg official list</a>. Not every card has been tested. These <a href="https://github.com/keylase/nvidia-patch">drivers</a> are recommended for Linux and Windows. Here is the official list of <a href="https://developer.nvidia.com/video-encode-decode-gpu-support-matrix">NVIDIA Graphics Cards</a> for supported codecs. Example of Ubuntu working with <a href="https://www.reddit.com/r/jellyfin/comments/amuyba/nvenc_nvdec_working_in_jellyfin_on_ubuntu_server/">NVENC</a>. H264 10-bit is <a href="https://devtalk.nvidia.com/default/topic/1039388/video-codec-and-optical-flow-sdk/is-there-nvidia-encoder-decoder-which-supports-hdr-10-bpp-for-avc-h-264-/">not supported</a> by NVIDIA acceleration. <strong>The minimum required driver version is: Linux: 418.30, Windows: 450.51.</strong></p>
<h2 id="vaapi">VAAPI</h2>
<p>List of supported codecs for <a href="https://wiki.archlinux.org/index.php/Hardware_video_acceleration#Comparison_tables">VAAPI</a>. Both Intel iGPU and AMD GPU can use VAAPI.</p>
<div class="NOTE"><h5>Note</h5><p>AMD GPU requires open source driver Mesa 20.1 or higher to support hardware decoding HEVC.</p>
</div>
<h2 id="amd-amf">AMD AMF</h2>
<p>AMF is now available on Windows and Linux, but since AMD has not implemented the HW decoder and scaler in ffmpeg, the decoding speed may not be as expected. The closed source driver <code>amdgpu-pro</code> is required when using AMF on Linux.</p>
<div class="NOTE"><h5>Note</h5><p>Zen is CPU only. No hardware acceleration for any form of video decoding/encoding. You will need an APU or dGPU for hardware acceleration.</p>
</div>
<h2 id="intel-quicksync">Intel QuickSync</h2>
<p>Intel QSV Benchmarks on <a href="https://www.intel.com/content/www/us/en/cloud-computing/cloud-computing-quicksync-video-ffmpeg-white-paper.html">Linux</a>.</p>
<p>On Windows, you can use the DXVA2/D3D11VA libraries for decoding and the libmfx library for encoding.</p>
<div class="NOTE"><h5>Note</h5><p>To use QSV on Windows, please do not install Jellyfin as a system service.</p>
</div>
<p>Issues: <a href="https://trac.ffmpeg.org/ticket/7511">FFmpeg Windows version with QSV hwaccel fails over TERMINAL</a> and <a href="https://trac.ffmpeg.org/ticket/6827">Intel QSV: &quot;Failed to create Direct3D device&quot; on Core i7-7700K (Skylake) on Windows 10</a></p>
<p>CentOS may require <a href="https://www.getpagespeed.com/server-setup/how-to-enable-intel-hardware-acceleration-for-video-playback-in-rhel-centos-8">additional drivers</a> for QSV.</p>
<p>Here&#39;s <a href="https://github.com/Artiume/jellyfin-docs/blob/master/general/wiki/main.md">additional information</a> to learn more.</p>
<p>If your Jellyfin server does not support hardware acceleration, but you have another machine that does, you can leverage <a href="https://github.com/joshuaboniface/rffmpeg">rffmpeg</a> to delegate the transcoding to another machine. Currently Linux-only and requires SSH between the machines, as well as shared storage both for media and for the Jellyfin data directory.</p>
<h2 id="enabling-hardware-acceleration">Enabling Hardware Acceleration</h2>
<p>Hardware acceleration options can be found in the Admin Dashboard under the <strong>Transcoding</strong> section of the <strong>Playback</strong> tab. Select a valid hardware acceleration option from the drop-down menu, indicate a device if applicable, and check <code>enable hardware encoding</code> to enable encoding as well as decoding, if your hardware supports this.</p>
<p>The hardware acceleration is available immediately for media playback. No server restart is required.</p>
<h2 id="setup">Setup</h2>
<p>Each hardware acceleration type, as well as each Jellyfin installation type, requires different setup options before it can be used. It is always best to consult the FFMpeg documentation on the acceleration type you choose for the latest information.</p>
<h3 id="acceleration-on-docker">Acceleration on Docker</h3>
<p>In order to use hardware acceleration in Docker, the devices must be passed to the container. To see what video devices are available, you can run <code>sudo lshw -c video</code> or <code>vainfo</code> on your machine.</p>
<div class="NOTE"><h5>Note</h5><p><a href="https://github.com/docker/compose/issues/6691">NVIDIA GPUs</a> aren&#39;t currently supported in docker-compose.</p>
</div>
<p>You can use <code>docker run</code> to start the server with a command like the one below.</p>
<pre><code class="lang-sh">docker run -d \
 --volume /path/to/config:/config \
 --volume /path/to/cache:/cache \
 --volume /path/to/media:/media \
 --user 1000:1000 \
 --net=host \
 --restart=unless-stopped \
 --device /dev/dri/renderD128:/dev/dri/renderD128 \
 --device /dev/dri/card0:/dev/dri/card0 \
 jellyfin/jellyfin
</code></pre><p>Alternatively, you can use docker-compose with a configuration file so you don&#39;t need to run a long command every time you restart your server.</p>
<pre><code class="lang-yaml">version: &quot;3&quot;
services:
  jellyfin:
    image: jellyfin/jellyfin
    user: 1000:1000
    network_mode: &quot;host&quot;
    volumes:
      - /path/to/config:/config
      - /path/to/cache:/cache
      - /path/to/media:/media
    devices:
      # VAAPI Devices
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
      # RPi 4
      - /dev/vchiq:/dev/vchiq
</code></pre><h3 id="debian-docker-nvidia">Debian Docker Nvidia</h3>
<p>In order to achieve hardware acceleration using docker, several steps are required.</p>
<p>Prerequisites:</p>
<p><a href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)">NVIDIA Docker Installation</a></p>
<ul>
<li>GNU/Linux x86_64 with kernel version &gt; 3.10</li>
<li>Docker &gt;= 1.12</li>
<li>NVIDIA GPU with Architecture &gt; Fermi (2.1)</li>
<li>NVIDIA drivers &gt;= 418.30</li>
</ul>
<p>Confirm that your GPU shows up with this command.</p>
<pre><code class="lang-sh">lspci | grep VGA
</code></pre><p>Update your host so there is no chance of outdated software causing issues.</p>
<pre><code class="lang-sh">apt-get update &amp;&amp; apt-get dist-upgrade -y
</code></pre><p>Install curl which will be used to download the required files.</p>
<pre><code class="lang-sh">apt-get install curl
</code></pre><p>Edit <code>sources.list</code> in <code>/etc/apt/sources.list</code> and add <code>non-free contrib</code> to each source as required.</p>
<pre><code class="lang-data">deb http://deb.debian.org/debian/ buster main
</code></pre><p>The line above should be modified to match the following line as an example.</p>
<pre><code class="lang-data">deb http://deb.debian.org/debian/ buster main non-free contrib
</code></pre><p>Download and add the sources for the Nvidia docker container.</p>
<pre><code class="lang-sh">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
</code></pre><p>Update your package list again to download the latest software available from the new repository.</p>
<pre><code class="lang-sh">apt-get update
</code></pre><p>Install linux-headers with the following command.</p>
<pre><code class="lang-sh">apt-get install linux-headers-$(uname -r | sed &#39;s/[^-]*-[^-]*-//&#39;)
</code></pre><p>Alternatively, Install linux-headers using backports.</p>
<pre><code class="lang-sh">distribution=$(. /etc/*-release;echo $VERSION_CODENAME)
apt-get install -t $distribution-backports linux-headers-$(uname -r | sed &#39;s/[^-]*-[^-]*-//&#39;)
</code></pre><p>Install Nvidia docker2 from the repository.</p>
<pre><code class="lang-sh">apt-get install nvidia-docker2
</code></pre><p>When prompted to choose to keep or install the maintainer package file type <code>y</code> to install the maintainer version.</p>
<p>After the install you may want to add nvidia as default runtime: editing <code>/etc/docker/daemon.json</code> like this:</p>
<pre><code class="lang-json">{
    &quot;default-runtime&quot;: &quot;nvidia&quot;,
    &quot;runtimes&quot;: {
        &quot;nvidia&quot;: {
            &quot;path&quot;: &quot;nvidia-container-runtime&quot;,
            &quot;runtimeArgs&quot;: []
        }
    }
}
</code></pre><p>Restart any docker services currently running.</p>
<pre><code class="lang-sh">sudo pkill -SIGHUP docker
</code></pre><p>Install nvidia drivers and dependencies.</p>
<pre><code class="lang-sh">distribution=$(. /etc/*-release;echo $VERSION_CODENAME)
apt-get install -t $distribution-backports nvidia-driver libnvcuvid1 libnvidia-encode1 libcuda1 nvidia-smi
</code></pre><p>Reboot your host to apply all changes.</p>
<pre><code class="lang-sh">reboot now
</code></pre><p>Validate that your driver and docker are correctly set up with this driver test.</p>
<pre><code class="lang-sh">nvidia-smi
docker run --gpus 0 nvidia/cuda:9.0-base nvidia-smi
</code></pre><p>Validate access to needed ressources from host and docker.</p>
<pre><code class="lang-sh">ldconfig -p | grep cuvid
ldconfig -p | grep libnvidia-encode.so.1
</code></pre><p>Start your container adding those environement parameters.</p>
<pre><code class="lang-sh">-e NVIDIA_DRIVER_CAPABILITIES=all \
-e NVIDIA_VISIBLE_DEVICES=all \
--runtime=nvidia \
--gpus all \
</code></pre><p>A complete run command would look like this.</p>
<pre><code class="lang-sh">docker run -d \
 --name=jellyfin \
 -e NVIDIA_DRIVER_CAPABILITIES=all \
 -e NVIDIA_VISIBLE_DEVICES=all \
 --gpus all \
 --runtime=nvidia \
 -p 8096:8096 \
 -p 8920:8920 \
 -v /config:/config \
 -v /media:/media \
 -v /cache:/cache \
 --restart unless-stopped \
 jellyfin/jellyfin
</code></pre><p>There are some special steps when running with the following option.</p>
<pre><code class="lang-sh">--user 1000:1000
</code></pre><p>You may need to add this user to the video group.</p>
<pre><code class="lang-sh">usermod -aG video user
</code></pre><p>Once the container is started you can again validate access to the host resources.</p>
<pre><code class="lang-sh">docker exec -it jellyfin nvidia-smi
</code></pre><p>If you get driver information, everything is fine but if you get an error like <code>couldn&#39;t find libnvidia-ml.so library in your system</code> you need to run the following command.</p>
<pre><code class="lang-sh">docker exec -it jellyfin ldconfig
</code></pre><p>After that, you should ensure the Nvidia driver loads correctly.</p>
<p>Now go to the Jellyfin playback settings, enable Nvidia NVENC and select your target codecs depending on what your GPU supports. Try to play any file needing a transcode by changing the bitrate.</p>
<p>Check the transcode logs to make sure everything is working properly.</p>
<pre><code class="lang-data">Stream #0:0 -&gt; #0:0 (h264 (h264_cuvid) -&gt; h264 (h264_nvenc))
Stream #0:2 -&gt; #0:1 (ac3 (native) -&gt; aac (native))
</code></pre><h3 id="configuring-opencl-accelerated-tone-mapping">Configuring OpenCL accelerated Tone mapping</h3>
<p>Currently, Tone mapping with Nvidia NVENC, AMD AMF and Intel VAAPI is through OpenCL image support.</p>
<table>
<thead>
<tr>
<th>OS/Platform</th>
<th>NVIDIA NVENC</th>
<th>AMD AMF</th>
<th>Intel VAAPI</th>
<th>AMD VAAPI</th>
<th>Intel QSV</th>
<th>Software</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux</td>
<td>OK</td>
<td>OK</td>
<td>OK</td>
<td>planned</td>
<td>planned</td>
<td>planned</td>
</tr>
<tr>
<td>Windows</td>
<td>OK</td>
<td>OK</td>
<td>N/A</td>
<td>N/A</td>
<td>planned</td>
<td>planned</td>
</tr>
<tr>
<td>Docker</td>
<td>OK</td>
<td>untested</td>
<td>OK</td>
<td>planned</td>
<td>planned</td>
<td>planned</td>
</tr>
</tbody>
</table>
<div class="NOTE"><h5>Note</h5><p>Make sure the hardware acceleration is well configured before reading this section.</p>
</div>
<ol>
<li><p>On Windows: Install the latest Nvidia or AMD drivers.</p>
</li>
<li><p>On Linux or docker:</p>
<ul>
<li><p>For Nvidia cards, install <code>nvidia-opencl-icd</code> on Debian/Ubuntu. Install <code>opencl-nvidia</code> on Arch.</p>
<pre><code class="lang-sh">sudo apt update
sudo apt install nvidia-opencl-icd
</code></pre><pre><code class="lang-sh">sudo pacman -Sy opencl-nvidia
</code></pre></li>
<li><p>For AMD cards, install <code>amdgpu-pro</code> with opencl arguments. See <strong>Configuring AMD AMF encoding on Ubuntu 18.04 or 20.04 LTS</strong> for more details.</p>
<pre><code class="lang-sh">sudo ./amdgpu-pro-install -y --opencl=pal,legacy
</code></pre></li>
<li><p>For Intel iGPUs, follow the instructions from <a href="https://github.com/intel/compute-runtime/releases">intel-compute-runtime</a>. If you are using the docker image from jellyfin/jellyfin, this step can be skipped.</p>
</li>
</ul>
<div class="NOTE"><h5>Note</h5><p>Tone mapping on Intel VAAPI needs an iGPU that supports 10-bit decoding.
Do not use the <code>intel-opencl-icd</code> package from the repository since they were not build with RELEASE_WITH_REGKEYS enabled for P010 pixel interop flags.</p>
</div>
</li>
<li><p>Check the OpenCL device status. You will see corresponding vendor name if it goes well.</p>
<ul>
<li><p>Use <code>clinfo</code>: Install <code>clinfo</code> before using it. <code>sudo apt update &amp;&amp; sudo apt install clinfo -y</code> on Debian/Ubuntu or <code>sudo pacman -Sy clinfo</code> on Arch. Then <code>sudo clinfo</code></p>
</li>
<li><p>Use <code>jellyfin-ffmpeg</code>: <code>/usr/lib/jellyfin-ffmpeg/ffmpeg -v debug -init_hw_device opencl</code></p>
</li>
</ul>
</li>
</ol>
<h3 id="configuring-vaapi-acceleration-on-debianubuntu-from-deb-packages">Configuring VAAPI acceleration on Debian/Ubuntu from <code>.deb</code> packages</h3>
<p>Configuring VAAPI on Debian/Ubuntu requires some additional configuration to ensure permissions are correct.</p>
<p>To check information about VAAPI on your system install and run <code>vainfo</code> from the command line.</p>
<div class="NOTE"><h5>Note</h5><p>For Intel Comet Lake or newer iGPUs, the legacy i965 VAAPI driver is incompatible with your hardware.
Please follow the instructions from <strong>Configuring Intel QuickSync(QSV) on Debian/Ubuntu</strong> to get the newer iHD driver.</p>
</div>
<ol>
<li><p>Configure VAAPI for your system by following the <a href="https://wiki.archlinux.org/index.php/Hardware_video_acceleration">relevant documentation</a>. Verify that a <code>render</code> device is now present in <code>/dev/dri</code>, and note the permissions and group available to write to it, in this case <code>render</code>:</p>
<pre><code class="lang-sh">$ ls -l /dev/dri
total 0
drwxr-xr-x 2 root root        100 Apr 13 16:37 by-path
crw-rw---- 1 root video  226,   0 Apr 13 16:37 card0
crw-rw---- 1 root video  226,   1 Apr 13 16:37 card1
crw-rw---- 1 root render 226, 128 Apr 13 16:37 renderD128
</code></pre><div class="NOTE"><h5>Note</h5><p>On some releases, the group may be <code>video</code> instead of <code>render</code>.</p>
</div>
</li>
<li><p>Add the Jellyfin service user to the above group to allow Jellyfin&#39;s FFMpeg process access to the device, and restart Jellyfin.</p>
<pre><code class="lang-sh">sudo usermod -aG render jellyfin
sudo systemctl restart jellyfin
</code></pre></li>
<li><p>Configure VAAPI acceleration in the &quot;Transcoding&quot; page of the Admin Dashboard. Enter the <code>/dev/dri/renderD128</code> device above as the <code>VA API Device</code> value.</p>
</li>
<li><p>Watch a movie, and verify that transcoding is occurring by watching the <code>ffmpeg-transcode-*.txt</code> logs under <code>/var/log/jellyfin</code> and using <code>radeontop</code> or similar tools.</p>
</li>
</ol>
<h3 id="configuring-intel-quicksyncqsv-on-debianubuntu">Configuring Intel QuickSync(QSV) on Debian/Ubuntu</h3>
<ol>
<li><p>QSV is based on VAAPI device on Linux, so please confirm whether you have completed the VAAPI configuration first.</p>
</li>
<li><p>Make sure that <code>jellyfin-ffmpeg 4.3.1-1</code> or higher is installed.</p>
</li>
<li><p>Install the non-free iHD driver.</p>
<pre><code class="lang-bash">sudo apt update
sudo apt install vainfo intel-media-va-driver-non-free -y
</code></pre><div class="NOTE"><h5>Note</h5><p>For Intel Comet Lake or newer iGPUs, make sure the version of <code>intel-media-va-driver-non-free</code> &gt;= 19.1.
<code>intel-media-va-driver-non-free</code> is avaliable from apt since Debian buster and Ubuntu 19.04. Otherwise you have to build from source.</p>
</div>
</li>
<li><p>Verify iHD driver using <code>vainfo</code>. You will find <code>iHD</code> from the result if it goes well.</p>
<pre><code class="lang-bash">vainfo | grep iHD
</code></pre></li>
<li><p>If you want to uninstall iHD driver and fallback to i965 driver.</p>
<pre><code class="lang-bash">sudo apt remove intel-media-va-driver intel-media-va-driver-non-free -y
</code></pre></li>
<li><p>QSV in docker. Due to incompatible licenses, we will not integrate non-free drivers in the docker image. You need to perform the above operations manually in docker and add <code>--privileged</code> to the docker configuration.</p>
</li>
</ol>
<h3 id="lxc-or-lxd-container">LXC or LXD Container</h3>
<p>This has been tested with LXC 3.0 and may or may not work with older versions.</p>
<p>Follow the steps above to add the jellyfin user to the <code>video</code> or <code>render</code> group, depending on your circumstances.</p>
<ol>
<li><p>Add your GPU to the container.</p>
<pre><code class="lang-sh">lxc config device add &lt;container name&gt; gpu gpu gid=&lt;gid of your video or render group&gt;
</code></pre></li>
<li><p>Make sure you have the card within the container:</p>
<pre><code class="lang-sh">$ lxc exec jellyfin -- ls -l /dev/dri
total 0
crw-rw---- 1 root video 226,   0 Jun  4 02:13 card0
crw-rw---- 1 root video 226,   0 Jun  4 02:13 controlD64
crw-rw---- 1 root video 226, 128 Jun  4 02:13 renderD128
</code></pre></li>
<li><p>Configure Jellyfin to use video acceleration and point it at the right device if the default option is wrong.</p>
</li>
<li><p>Try and play a video that requires transcoding and run the following, you should get a hit.</p>
<pre><code class="lang-sh">ps aux | grep ffmpeg | grep accel
</code></pre></li>
<li><p>You can also try playing a video that requires transcoding, and if it plays you&#39;re good.</p>
</li>
</ol>
<p>Useful Resources:</p>
<ul>
<li><a href="https://github.com/lxc/lxd/blob/master/doc/instances.md#type-gpu">LXD Documentation - GPU instance configuration</a></li>
<li><a href="https://stgraber.org/2017/03/21/cuda-in-lxd/">NVidia CUDA inside a LXD container</a></li>
</ul>
<h3 id="configuring-amd-amf-encoding-on-ubuntu-1804-or-2004-lts">Configuring AMD AMF encoding on Ubuntu 18.04 or 20.04 LTS</h3>
<ol>
<li><p>Install <code>amdgpu-pro</code> closed source graphics driver by following the <a href="https://amdgpu-install.readthedocs.io/en/latest/">installation instructions</a>.</p>
</li>
<li><p>Then install <code>amf-amdgpu-pro</code>:</p>
<pre><code class="lang-bash">sudo apt install amf-amdgpu-pro
</code></pre></li>
<li><p>Make sure your <code>jellyfin-ffmpeg</code> or <code>ffmpeg</code> contains <code>h264_amf</code> encoder:</p>
<pre><code class="lang-bash">$ cd /usr/lib/jellyfin-ffmpeg/
$ ./ffmpeg -encoders | grep h264_amf
V..... h264_amf             AMD AMF H.264 Encoder (codec h264)
</code></pre><div class="NOTE"><h5>Note</h5><p>If not contain, update your <code>jellyfin-ffmpeg</code> to the latest version and try again.</p>
</div>
<p>For compiling ffmpeg with AMF library, refer to <a href="https://www.ffmpeg.org/general.html#AMD-AMF_002fVCE">this page</a>.</p>
</li>
<li><p>Choose AMD AMF video acceleration in Jellyfin and check the <code>Enable hardware encoding</code> option.</p>
</li>
<li><p>Watch a movie, then verify that <code>h264_amf</code> encoder is working by watching the <code>ffmpeg-transcode-*.txt</code> transcoding logs under <code>/var/log/jellyfin</code> and using <code>radeontop</code> or similar tools.</p>
</li>
</ol>
<h3 id="configuring-amd-amf-encoding-on-arch-linux">Configuring AMD AMF encoding on Arch Linux</h3>
<p>AMD does not provide official <code>amdgpu-pro</code> driver support for Arch Linux, but fortunately, a third-party packaged <code>amdgpu-pro-installer</code> is provided in the archlinux user repository.</p>
<ol>
<li><p>Clone <a href="https://aur.archlinux.org/pkgbase/amdgpu-pro-installer/">this repository</a> using <code>git</code>:</p>
<pre><code class="lang-bash">git clone https://aur.archlinux.org/amdgpu-pro-installer.git
</code></pre></li>
<li><p>Enter that folder and make the installation package and install it:</p>
<pre><code class="lang-bash">cd amdgpu-pro-installer
makepkg -si
</code></pre></li>
<li><p>Go to step 3 of <strong>on Ubuntu 18.04 or 20.04 LTS</strong> above.</p>
</li>
</ol>
<h3 id="configuring-omx-on-raspberry-pi-3-and-4-running-raspberry-pi-os">Configuring OMX on Raspberry Pi 3 and 4 running Raspberry Pi OS</h3>
<ol>
<li><p>Add the Jellyfin service user to the video group to allow Jellyfin&#39;s FFMpeg process access to the encoder, and restart Jellyfin.</p>
<pre><code class="lang-sh">sudo usermod -aG video jellyfin
sudo systemctl restart jellyfin
</code></pre><div class="NOTE"><h5>Note</h5><p>If you are using a Raspberry Pi 4, you might need to run <code>sudo rpi-update</code> for kernel and firmware updates.</p>
</div>
</li>
<li><p>Choose <code>OpenMAX OMX</code> as the Hardware acceleration on the Transcoding tab of the Server Dashboard.</p>
</li>
<li><p>Change the amount of memory allocated to the GPU. The GPU can&#39;t handle accelerated decoding and encoding simultaneously.</p>
<pre><code class="lang-sh">sudo nano /boot/config.txt
</code></pre><p> For RPi4, add the line <code>gpu_mem=320</code> <a href="https://www.raspberrypi.org/documentation/configuration/config-txt/">See more Here</a></p>
<p> For RPi3, add the line <code>gpu_mem=256</code></p>
<p> You can set any value, but 320 is recommended amount for <a href="https://github.com/CoreELEC/CoreELEC/blob/coreelec-9.2/projects/RPi/devices/RPi4/config/config.txt">4K HEVC</a>.</p>
<p> Verify the split between CPU and GPU memory:</p>
<pre><code class="lang-sh">vcgencmd get_mem arm &amp;&amp; vcgencmd get_mem gpu
</code></pre><p> Monitor the temperature and clock speed of the CPU:</p>
<pre><code class="lang-sh">vcgencmd measure_temp &amp;&amp; vcgencmd measure_clock arm
</code></pre></li>
</ol>
<div class="NOTE"><h5>Note</h5><p>RPi4 currently doesn&#39;t support HWA HEVC decoding, only encode and decode H.264. <a href="https://www.jeffgeerling.com/blog/2019/raspberry-pi-4-needs-fan-heres-why-and-how-you-can-add-one">Active cooling</a> is required, passive cooling is insufficient for transcoding. Only Raspbian OS works so far. For docker, only the linuxserver image works. For more tips see <a href="https://www.reddit.com/r/jellyfin/comments/ei6ew6/rpi4_hardware_acceleration_guide/">here</a>.</p>
</div>
<div class="NOTE"><h5>Note</h5><p>For RPi3 in testing, transcoding was not working fast enough to run in real time because the video was being resized.</p>
</div>
<h3 id="configuring-vaapi-on-raspberry-pi-3-and-4-running-ubuntu-server">Configuring VAAPI on Raspberry Pi 3 and 4 running Ubuntu server</h3>
<ol>
<li><p>Enable the <code>render</code> device on your Pi.</p>
<ul>
<li><p>If you are using Raspberry Pi 3 add this to the <code>/boot/firmware/usercfg.txt</code> file.</p>
<pre><code class="lang-sh">gpu_mem=256
dtoverlay=vc4-kms-v3d
</code></pre></li>
<li><p>Or if you are a Raspberry Pi 4 user add this to the <code>/boot/firmware/usercfg.txt</code> file.</p>
<pre><code class="lang-sh">gpu_mem=320
dtoverlay=vc4-kms-v3d-pi4
</code></pre><p>Then save the file and do a reboot. Verify that a <code>render</code> device is now present in <code>/dev/dri</code>.</p>
<pre><code class="lang-sh">$ ls -l /dev/dri
total 0
drwxr-xr-x 2 root root        100 Oct 13 16:00 by-path
crw-rw---- 1 root video  226,   0 Oct 13 16:00 card0
crw-rw---- 1 root video  226,   1 Oct 13 16:00 card1
crw-rw---- 1 root render 226, 128 Oct 13 16:00 renderD128
</code></pre></li>
</ul>
</li>
<li><p>Add Jellyfin service user to the <code>render</code> group to allow Jellyfin&#39;s FFMpeg process access to the device, and restart Jellyfin.</p>
<pre><code class="lang-sh">sudo usermod -aG render jellyfin
sudo systemctl restart jellyfin
</code></pre></li>
<li><p>Configure VAAPI acceleration in the <strong>Transcoding</strong> page of the Admin Dashboard. Enter the <code>/dev/dri/renderD128</code> device above as the <code>VA API</code> Device value.</p>
</li>
<li><p>Watch a movie, and verify that transcoding is working by watching <code>ffmpeg-transcode-*.txt</code> logs under <code>/var/log/jellyfin</code>.</p>
</li>
</ol>
<h2 id="verifying-transcodes">Verifying Transcodes</h2>
<p>To verify that you are using the proper libraries, run this command against your transcoding log. This can be found at Admin Dashboard &gt; Logs, and /var/log/jellyfin if instead via the repository.</p>
<pre><code class="lang-sh">grep -A2 &#39;Stream mapping:&#39; /var/log/jellyfin/ffmpeg-transcode-85a68972-7129-474c-9c5d-2d9949021b44.txt
</code></pre><p>This returned the following results.</p>
<pre><code class="lang-data">Stream mapping:
Stream #0:0 -&gt; #0:0 (hevc (native) -&gt; h264 (h264_omx))
Stream #0:1 -&gt; #0:1 (aac (native) -&gt; mp3 (libmp3lame))
</code></pre><p><code>Stream #0:0</code> used software (VAAPI Decode can also say native) to decode HEVC and used HWA to encode.</p>
<pre><code class="lang-data">Stream mapping:
Stream #0:0 -&gt; #0:0 (h264 (h264_mmal) -&gt; h264 (h264_omx))
Stream #0:1 -&gt; #0:1 (flac (native) -&gt; mp3 (libmp3lame))
</code></pre><p><code>Stream #0:0</code> used HWA for both. h264_mmal to decode and h264_omx to encode.</p>
</article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                  <li>
                    <a href="https://github.com/jellyfin/jellyfin-docs/blob/master/general/administration/hardware-acceleration.md/#L1" class="contribution-link">Improve this Doc</a>
                  </li>
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
              <!-- <p><a class="back-to-top" href="#top">Back to top</a><p> -->
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
            
            <span>Generated by <strong>DocFX</strong></span>
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../../styles/docfx.vendor.js"></script>
    <script type="text/javascript" src="../../styles/docfx.js"></script>
    <script type="text/javascript" src="../../styles/main.js"></script>
  </body>
</html>
